{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6a634123-c19d-411c-808e-3e825c217019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# titanic\n",
    "# KFoldを適用\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e41c7fe2-140e-4726-a983-bea9731606a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ・セット\n",
    "DATA_DIR = './data/'\n",
    "TRAIN_DATA = 'train.csv'\n",
    "DATA = pd.read_csv(os.path.join(DATA_DIR, TRAIN_DATA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1bd25602-92ba-48f0-9e39-b6159aa08217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
       "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "41e87dd3-eb24-4596-a76d-cd759b2c4887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 性別をダミー変数化\n",
    "DATA['Sex'] = DATA['Sex'].replace(['male', 'female'], [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e9046051-c042-46f1-9fbb-1fe5423ae799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA['Sex'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "bd9ae134-dd79-48f5-9d87-2372eac60e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name  Sex   Age  SibSp  Parch  \\\n",
       "0                            Braund, Mr. Owen Harris    0  22.0      1      0   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...    1  38.0      1      0   \n",
       "2                             Heikkinen, Miss. Laina    1  26.0      0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)    1  35.0      1      0   \n",
       "4                           Allen, Mr. William Henry    0  35.0      0      0   \n",
       "\n",
       "             Ticket     Fare Cabin Embarked  \n",
       "0         A/5 21171   7.2500   NaN        S  \n",
       "1          PC 17599  71.2833   C85        C  \n",
       "2  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3            113803  53.1000  C123        S  \n",
       "4            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d0c9881a-bacc-4c2d-b2ae-236f369c4ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "61449e61-b3b8-4f89-82bc-091c453f4e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# カテゴリカルデータをダミー変数化\n",
    "embarked_le = le.fit_transform(DATA['Embarked'].values)\n",
    "\n",
    "name_le = le.fit_transform(DATA['Name'].values)\n",
    "\n",
    "ticket_le = le.fit_transform(DATA['Ticket'].values)\n",
    "\n",
    "cabin_le = le.fit_transform(DATA['Cabin'].values)\n",
    "\n",
    "# ダミー変数化したデータを元データに適用\n",
    "data = DATA\n",
    "data['Embarked'] = embarked_le\n",
    "data['Name'] = name_le\n",
    "data['Ticket'] = ticket_le\n",
    "data['Cabin'] = cabin_le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "bfe943cd-fca4-4533-bee5-57d5158d71bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>523</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>147</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>190</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>596</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>353</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>669</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>147</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>272</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>472</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>147</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  Name  Sex   Age  SibSp  Parch  Ticket  \\\n",
       "0            1         0       3   108    0  22.0      1      0     523   \n",
       "1            2         1       1   190    1  38.0      1      0     596   \n",
       "2            3         1       3   353    1  26.0      0      0     669   \n",
       "3            4         1       1   272    1  35.0      1      0      49   \n",
       "4            5         0       3    15    0  35.0      0      0     472   \n",
       "\n",
       "      Fare  Cabin  Embarked  \n",
       "0   7.2500    147         2  \n",
       "1  71.2833     81         0  \n",
       "2   7.9250    147         2  \n",
       "3  53.1000     55         2  \n",
       "4   8.0500    147         2  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "72c26172-fe9c-447c-bf90-ce1b7599f8b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin            0\n",
       "Embarked         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2db94023-0e36-4356-bdfe-71df37d1d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 年齢の欠損処理\n",
    "# 平均で埋めて、データフレームに適用\n",
    "data['Age'] = data['Age'].fillna(data['Age'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "035fe85d-2816-4794-a352-218052b1958b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId    0\n",
       "Survived       0\n",
       "Pclass         0\n",
       "Name           0\n",
       "Sex            0\n",
       "Age            0\n",
       "SibSp          0\n",
       "Parch          0\n",
       "Ticket         0\n",
       "Fare           0\n",
       "Cabin          0\n",
       "Embarked       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "310947ed-ea8b-4f9d-8254-b91541d37818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目的変数と説明変数に分割\n",
    "y = data['Survived']\n",
    "\n",
    "# y = np.array([0, 0, 1, 1])\n",
    "X = data.drop(columns={'Survived'})\n",
    "\n",
    "# 欠損値確認\n",
    "# X.isnull().sum()\n",
    "\n",
    "# Ageの欠損値を平均で穴埋め\n",
    "X['Age'] = X['Age'].fillna(X['Age'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "fa79a4dc-b3e9-4118-9770-fa88d911e3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=3, random_state=None, shuffle=False)\n",
      "TRAIN: [297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314\n",
      " 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332\n",
      " 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350\n",
      " 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368\n",
      " 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386\n",
      " 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404\n",
      " 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422\n",
      " 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440\n",
      " 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458\n",
      " 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476\n",
      " 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494\n",
      " 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512\n",
      " 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530\n",
      " 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548\n",
      " 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566\n",
      " 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584\n",
      " 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602\n",
      " 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620\n",
      " 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638\n",
      " 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656\n",
      " 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674\n",
      " 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692\n",
      " 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710\n",
      " 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728\n",
      " 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746\n",
      " 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764\n",
      " 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782\n",
      " 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800\n",
      " 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818\n",
      " 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836\n",
      " 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854\n",
      " 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872\n",
      " 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890] TEST: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296]\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 594 595 596 597 598 599 600 601 602\n",
      " 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620\n",
      " 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638\n",
      " 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656\n",
      " 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674\n",
      " 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692\n",
      " 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710\n",
      " 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728\n",
      " 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746\n",
      " 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764\n",
      " 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782\n",
      " 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800\n",
      " 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818\n",
      " 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836\n",
      " 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854\n",
      " 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872\n",
      " 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890] TEST: [297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314\n",
      " 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332\n",
      " 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350\n",
      " 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368\n",
      " 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386\n",
      " 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404\n",
      " 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422\n",
      " 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440\n",
      " 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458\n",
      " 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476\n",
      " 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494\n",
      " 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512\n",
      " 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530\n",
      " 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548\n",
      " 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566\n",
      " 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584\n",
      " 585 586 587 588 589 590 591 592 593]\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
      " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
      " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
      " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
      " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
      " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
      " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
      " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
      " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593] TEST: [594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611\n",
      " 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629\n",
      " 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647\n",
      " 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665\n",
      " 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683\n",
      " 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701\n",
      " 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719\n",
      " 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737\n",
      " 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755\n",
      " 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773\n",
      " 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791\n",
      " 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809\n",
      " 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827\n",
      " 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845\n",
      " 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863\n",
      " 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881\n",
      " 882 883 884 885 886 887 888 889 890]\n"
     ]
    }
   ],
   "source": [
    "# KFoldで交差検証\n",
    "kf = KFold(n_splits=3)\n",
    "kf.get_n_splits(X, y)\n",
    "\n",
    "print(kf)\n",
    "\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train1, X_test1 = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train1, y_test1 = y.iloc[train_index], y.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aedc0f-426a-4125-aac3-02072988caf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "54aa0b30-2722-4eb0-b2d6-4b1be16ea5e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((594, 11), (594,), (297, 11), (297,))"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1.shape, y_train1.shape, X_test1.shape, y_test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "50a18946-7d77-41ce-a6ca-f07a88c378dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mLogisticRegressionCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mCs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfit_intercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdual\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrefit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mintercept_scaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0ml1_ratios\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Logistic Regression CV (aka logit, MaxEnt) classifier.\n",
       "\n",
       "See glossary entry for :term:`cross-validation estimator`.\n",
       "\n",
       "This class implements logistic regression using liblinear, newton-cg, sag\n",
       "of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2\n",
       "regularization with primal formulation. The liblinear solver supports both\n",
       "L1 and L2 regularization, with a dual formulation only for the L2 penalty.\n",
       "Elastic-Net penalty is only supported by the saga solver.\n",
       "\n",
       "For the grid of `Cs` values and `l1_ratios` values, the best hyperparameter\n",
       "is selected by the cross-validator\n",
       ":class:`~sklearn.model_selection.StratifiedKFold`, but it can be changed\n",
       "using the :term:`cv` parameter. The 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
       "solvers can warm-start the coefficients (see :term:`Glossary<warm_start>`).\n",
       "\n",
       "Read more in the :ref:`User Guide <logistic_regression>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "Cs : int or list of floats, default=10\n",
       "    Each of the values in Cs describes the inverse of regularization\n",
       "    strength. If Cs is as an int, then a grid of Cs values are chosen\n",
       "    in a logarithmic scale between 1e-4 and 1e4.\n",
       "    Like in support vector machines, smaller values specify stronger\n",
       "    regularization.\n",
       "\n",
       "fit_intercept : bool, default=True\n",
       "    Specifies if a constant (a.k.a. bias or intercept) should be\n",
       "    added to the decision function.\n",
       "\n",
       "cv : int or cross-validation generator, default=None\n",
       "    The default cross-validation generator used is Stratified K-Folds.\n",
       "    If an integer is provided, then it is the number of folds used.\n",
       "    See the module :mod:`sklearn.model_selection` module for the\n",
       "    list of possible cross-validation objects.\n",
       "\n",
       "    .. versionchanged:: 0.22\n",
       "        ``cv`` default value if None changed from 3-fold to 5-fold.\n",
       "\n",
       "dual : bool, default=False\n",
       "    Dual or primal formulation. Dual formulation is only implemented for\n",
       "    l2 penalty with liblinear solver. Prefer dual=False when\n",
       "    n_samples > n_features.\n",
       "\n",
       "penalty : {'l1', 'l2', 'elasticnet'}, default='l2'\n",
       "    Used to specify the norm used in the penalization. The 'newton-cg',\n",
       "    'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n",
       "    only supported by the 'saga' solver.\n",
       "\n",
       "scoring : str or callable, default=None\n",
       "    A string (see model evaluation documentation) or\n",
       "    a scorer callable object / function with signature\n",
       "    ``scorer(estimator, X, y)``. For a list of scoring functions\n",
       "    that can be used, look at :mod:`sklearn.metrics`. The\n",
       "    default scoring option used is 'accuracy'.\n",
       "\n",
       "solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'\n",
       "\n",
       "    Algorithm to use in the optimization problem.\n",
       "\n",
       "    - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n",
       "      'saga' are faster for large ones.\n",
       "    - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
       "      handle multinomial loss; 'liblinear' is limited to one-versus-rest\n",
       "      schemes.\n",
       "    - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas\n",
       "      'liblinear' and 'saga' handle L1 penalty.\n",
       "    - 'liblinear' might be slower in LogisticRegressionCV because it does\n",
       "      not handle warm-starting.\n",
       "\n",
       "    Note that 'sag' and 'saga' fast convergence is only guaranteed on\n",
       "    features with approximately the same scale. You can preprocess the data\n",
       "    with a scaler from sklearn.preprocessing.\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "       Stochastic Average Gradient descent solver.\n",
       "    .. versionadded:: 0.19\n",
       "       SAGA solver.\n",
       "\n",
       "tol : float, default=1e-4\n",
       "    Tolerance for stopping criteria.\n",
       "\n",
       "max_iter : int, default=100\n",
       "    Maximum number of iterations of the optimization algorithm.\n",
       "\n",
       "class_weight : dict or 'balanced', default=None\n",
       "    Weights associated with classes in the form ``{class_label: weight}``.\n",
       "    If not given, all classes are supposed to have weight one.\n",
       "\n",
       "    The \"balanced\" mode uses the values of y to automatically adjust\n",
       "    weights inversely proportional to class frequencies in the input data\n",
       "    as ``n_samples / (n_classes * np.bincount(y))``.\n",
       "\n",
       "    Note that these weights will be multiplied with sample_weight (passed\n",
       "    through the fit method) if sample_weight is specified.\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "       class_weight == 'balanced'\n",
       "\n",
       "n_jobs : int, default=None\n",
       "    Number of CPU cores used during the cross-validation loop.\n",
       "    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
       "    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
       "    for more details.\n",
       "\n",
       "verbose : int, default=0\n",
       "    For the 'liblinear', 'sag' and 'lbfgs' solvers set verbose to any\n",
       "    positive number for verbosity.\n",
       "\n",
       "refit : bool, default=True\n",
       "    If set to True, the scores are averaged across all folds, and the\n",
       "    coefs and the C that corresponds to the best score is taken, and a\n",
       "    final refit is done using these parameters.\n",
       "    Otherwise the coefs, intercepts and C that correspond to the\n",
       "    best scores across folds are averaged.\n",
       "\n",
       "intercept_scaling : float, default=1\n",
       "    Useful only when the solver 'liblinear' is used\n",
       "    and self.fit_intercept is set to True. In this case, x becomes\n",
       "    [x, self.intercept_scaling],\n",
       "    i.e. a \"synthetic\" feature with constant value equal to\n",
       "    intercept_scaling is appended to the instance vector.\n",
       "    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
       "\n",
       "    Note! the synthetic feature weight is subject to l1/l2 regularization\n",
       "    as all other features.\n",
       "    To lessen the effect of regularization on synthetic feature weight\n",
       "    (and therefore on the intercept) intercept_scaling has to be increased.\n",
       "\n",
       "multi_class : {'auto, 'ovr', 'multinomial'}, default='auto'\n",
       "    If the option chosen is 'ovr', then a binary problem is fit for each\n",
       "    label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
       "    across the entire probability distribution, *even when the data is\n",
       "    binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
       "    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
       "    and otherwise selects 'multinomial'.\n",
       "\n",
       "    .. versionadded:: 0.18\n",
       "       Stochastic Average Gradient descent solver for 'multinomial' case.\n",
       "    .. versionchanged:: 0.22\n",
       "        Default changed from 'ovr' to 'auto' in 0.22.\n",
       "\n",
       "random_state : int, RandomState instance, default=None\n",
       "    Used when `solver='sag'`, 'saga' or 'liblinear' to shuffle the data.\n",
       "    Note that this only applies to the solver and not the cross-validation\n",
       "    generator. See :term:`Glossary <random_state>` for details.\n",
       "\n",
       "l1_ratios : list of float, default=None\n",
       "    The list of Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``.\n",
       "    Only used if ``penalty='elasticnet'``. A value of 0 is equivalent to\n",
       "    using ``penalty='l2'``, while 1 is equivalent to using\n",
       "    ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a combination\n",
       "    of L1 and L2.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "classes_ : ndarray of shape (n_classes, )\n",
       "    A list of class labels known to the classifier.\n",
       "\n",
       "coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
       "    Coefficient of the features in the decision function.\n",
       "\n",
       "    `coef_` is of shape (1, n_features) when the given problem\n",
       "    is binary.\n",
       "\n",
       "intercept_ : ndarray of shape (1,) or (n_classes,)\n",
       "    Intercept (a.k.a. bias) added to the decision function.\n",
       "\n",
       "    If `fit_intercept` is set to False, the intercept is set to zero.\n",
       "    `intercept_` is of shape(1,) when the problem is binary.\n",
       "\n",
       "Cs_ : ndarray of shape (n_cs)\n",
       "    Array of C i.e. inverse of regularization parameter values used\n",
       "    for cross-validation.\n",
       "\n",
       "l1_ratios_ : ndarray of shape (n_l1_ratios)\n",
       "    Array of l1_ratios used for cross-validation. If no l1_ratio is used\n",
       "    (i.e. penalty is not 'elasticnet'), this is set to ``[None]``\n",
       "\n",
       "coefs_paths_ : ndarray of shape (n_folds, n_cs, n_features) or                    (n_folds, n_cs, n_features + 1)\n",
       "    dict with classes as the keys, and the path of coefficients obtained\n",
       "    during cross-validating across each fold and then across each Cs\n",
       "    after doing an OvR for the corresponding class as values.\n",
       "    If the 'multi_class' option is set to 'multinomial', then\n",
       "    the coefs_paths are the coefficients corresponding to each class.\n",
       "    Each dict value has shape ``(n_folds, n_cs, n_features)`` or\n",
       "    ``(n_folds, n_cs, n_features + 1)`` depending on whether the\n",
       "    intercept is fit or not. If ``penalty='elasticnet'``, the shape is\n",
       "    ``(n_folds, n_cs, n_l1_ratios_, n_features)`` or\n",
       "    ``(n_folds, n_cs, n_l1_ratios_, n_features + 1)``.\n",
       "\n",
       "scores_ : dict\n",
       "    dict with classes as the keys, and the values as the\n",
       "    grid of scores obtained during cross-validating each fold, after doing\n",
       "    an OvR for the corresponding class. If the 'multi_class' option\n",
       "    given is 'multinomial' then the same scores are repeated across\n",
       "    all classes, since this is the multinomial class. Each dict value\n",
       "    has shape ``(n_folds, n_cs`` or ``(n_folds, n_cs, n_l1_ratios)`` if\n",
       "    ``penalty='elasticnet'``.\n",
       "\n",
       "C_ : ndarray of shape (n_classes,) or (n_classes - 1,)\n",
       "    Array of C that maps to the best scores across every class. If refit is\n",
       "    set to False, then for each class, the best C is the average of the\n",
       "    C's that correspond to the best scores for each fold.\n",
       "    `C_` is of shape(n_classes,) when the problem is binary.\n",
       "\n",
       "l1_ratio_ : ndarray of shape (n_classes,) or (n_classes - 1,)\n",
       "    Array of l1_ratio that maps to the best scores across every class. If\n",
       "    refit is set to False, then for each class, the best l1_ratio is the\n",
       "    average of the l1_ratio's that correspond to the best scores for each\n",
       "    fold.  `l1_ratio_` is of shape(n_classes,) when the problem is binary.\n",
       "\n",
       "n_iter_ : ndarray of shape (n_classes, n_folds, n_cs) or (1, n_folds, n_cs)\n",
       "    Actual number of iterations for all classes, folds and Cs.\n",
       "    In the binary or multinomial cases, the first dimension is equal to 1.\n",
       "    If ``penalty='elasticnet'``, the shape is ``(n_classes, n_folds,\n",
       "    n_cs, n_l1_ratios)`` or ``(1, n_folds, n_cs, n_l1_ratios)``.\n",
       "\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.datasets import load_iris\n",
       ">>> from sklearn.linear_model import LogisticRegressionCV\n",
       ">>> X, y = load_iris(return_X_y=True)\n",
       ">>> clf = LogisticRegressionCV(cv=5, random_state=0).fit(X, y)\n",
       ">>> clf.predict(X[:2, :])\n",
       "array([0, 0])\n",
       ">>> clf.predict_proba(X[:2, :]).shape\n",
       "(2, 3)\n",
       ">>> clf.score(X, y)\n",
       "0.98...\n",
       "\n",
       "See Also\n",
       "--------\n",
       "LogisticRegression\n",
       "\u001b[0;31mFile:\u001b[0m           /data_01/script/script_python/.venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LogisticRegressionCV?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b14f41f7-2d57-4432-8023-dd072001e79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data_01/script/script_python/.venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.05, max_iter=300, random_state=0)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model1\n",
    "# ロジスティック回帰\n",
    "lr = LogisticRegression(random_state=0, max_iter=300, C=0.05)\n",
    "lr.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "97637bcd-87c3-4ecc-ba7b-a8f7b0684bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7878787878787878"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルの精度\n",
    "lr.score(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "debf0c70-b3f5-4f79-9400-8796ed6f41a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2\n",
    "# xgboost\n",
    "# xgbでモデリング\n",
    "xgb01 = xgb.XGBClassifier(\n",
    "    objective= \"binary:logistic\",\n",
    "    colsample_bytree= 0.8,\n",
    "    eta= 0.3,\n",
    "    eval_metric= 'logloss',\n",
    "    # lambda= 0,\n",
    "    learning_rate= 0.05,\n",
    "    max_depth=4,\n",
    "    n_estimators=20,\n",
    "    subsample= 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "2a8f3ee1-c3d3-4cf9-a924-9b9c2fc16435",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data_01/script/script_python/.venv/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.8, eta=0.3,\n",
       "              eval_metric='logloss', gamma=0, gpu_id=-1, importance_type='gain',\n",
       "              interaction_constraints='', learning_rate=0.05, max_delta_step=0,\n",
       "              max_depth=4, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=20, n_jobs=4,\n",
       "              num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "              scale_pos_weight=1, subsample=0.7, tree_method='exact',\n",
       "              validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb01.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "086b7d19-7e02-42ec-81e0-7b293052b2f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8720538720538721"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 精度検証\n",
    "xgb01.score(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f7d3489b-c32f-47d8-92b7-4b703f1d8498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model3\n",
    "# randomforest\n",
    "rf01 = RandomForestClassifier(random_state=0)\n",
    "\n",
    "rf01.fit(X_train1, y_train1)\n",
    "\n",
    "# 精度検証\n",
    "rf01.score(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9e55cf74-88d5-4d35-b866-524407dfb2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9932659932659933"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model4\n",
    "# randomforest\n",
    "rf02 = RandomForestClassifier(random_state=0, n_estimators=20, class_weight='balanced')\n",
    "\n",
    "rf02.fit(X_train1, y_train1)\n",
    "\n",
    "# 精度検証\n",
    "rf02.score(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "4939a820-783e-4bdc-b541-b4d135e0214a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mLogisticRegressionCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mCs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfit_intercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdual\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrefit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mintercept_scaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0ml1_ratios\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Logistic Regression CV (aka logit, MaxEnt) classifier.\n",
       "\n",
       "See glossary entry for :term:`cross-validation estimator`.\n",
       "\n",
       "This class implements logistic regression using liblinear, newton-cg, sag\n",
       "of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2\n",
       "regularization with primal formulation. The liblinear solver supports both\n",
       "L1 and L2 regularization, with a dual formulation only for the L2 penalty.\n",
       "Elastic-Net penalty is only supported by the saga solver.\n",
       "\n",
       "For the grid of `Cs` values and `l1_ratios` values, the best hyperparameter\n",
       "is selected by the cross-validator\n",
       ":class:`~sklearn.model_selection.StratifiedKFold`, but it can be changed\n",
       "using the :term:`cv` parameter. The 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
       "solvers can warm-start the coefficients (see :term:`Glossary<warm_start>`).\n",
       "\n",
       "Read more in the :ref:`User Guide <logistic_regression>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "Cs : int or list of floats, default=10\n",
       "    Each of the values in Cs describes the inverse of regularization\n",
       "    strength. If Cs is as an int, then a grid of Cs values are chosen\n",
       "    in a logarithmic scale between 1e-4 and 1e4.\n",
       "    Like in support vector machines, smaller values specify stronger\n",
       "    regularization.\n",
       "\n",
       "fit_intercept : bool, default=True\n",
       "    Specifies if a constant (a.k.a. bias or intercept) should be\n",
       "    added to the decision function.\n",
       "\n",
       "cv : int or cross-validation generator, default=None\n",
       "    The default cross-validation generator used is Stratified K-Folds.\n",
       "    If an integer is provided, then it is the number of folds used.\n",
       "    See the module :mod:`sklearn.model_selection` module for the\n",
       "    list of possible cross-validation objects.\n",
       "\n",
       "    .. versionchanged:: 0.22\n",
       "        ``cv`` default value if None changed from 3-fold to 5-fold.\n",
       "\n",
       "dual : bool, default=False\n",
       "    Dual or primal formulation. Dual formulation is only implemented for\n",
       "    l2 penalty with liblinear solver. Prefer dual=False when\n",
       "    n_samples > n_features.\n",
       "\n",
       "penalty : {'l1', 'l2', 'elasticnet'}, default='l2'\n",
       "    Used to specify the norm used in the penalization. The 'newton-cg',\n",
       "    'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n",
       "    only supported by the 'saga' solver.\n",
       "\n",
       "scoring : str or callable, default=None\n",
       "    A string (see model evaluation documentation) or\n",
       "    a scorer callable object / function with signature\n",
       "    ``scorer(estimator, X, y)``. For a list of scoring functions\n",
       "    that can be used, look at :mod:`sklearn.metrics`. The\n",
       "    default scoring option used is 'accuracy'.\n",
       "\n",
       "solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'\n",
       "\n",
       "    Algorithm to use in the optimization problem.\n",
       "\n",
       "    - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n",
       "      'saga' are faster for large ones.\n",
       "    - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
       "      handle multinomial loss; 'liblinear' is limited to one-versus-rest\n",
       "      schemes.\n",
       "    - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas\n",
       "      'liblinear' and 'saga' handle L1 penalty.\n",
       "    - 'liblinear' might be slower in LogisticRegressionCV because it does\n",
       "      not handle warm-starting.\n",
       "\n",
       "    Note that 'sag' and 'saga' fast convergence is only guaranteed on\n",
       "    features with approximately the same scale. You can preprocess the data\n",
       "    with a scaler from sklearn.preprocessing.\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "       Stochastic Average Gradient descent solver.\n",
       "    .. versionadded:: 0.19\n",
       "       SAGA solver.\n",
       "\n",
       "tol : float, default=1e-4\n",
       "    Tolerance for stopping criteria.\n",
       "\n",
       "max_iter : int, default=100\n",
       "    Maximum number of iterations of the optimization algorithm.\n",
       "\n",
       "class_weight : dict or 'balanced', default=None\n",
       "    Weights associated with classes in the form ``{class_label: weight}``.\n",
       "    If not given, all classes are supposed to have weight one.\n",
       "\n",
       "    The \"balanced\" mode uses the values of y to automatically adjust\n",
       "    weights inversely proportional to class frequencies in the input data\n",
       "    as ``n_samples / (n_classes * np.bincount(y))``.\n",
       "\n",
       "    Note that these weights will be multiplied with sample_weight (passed\n",
       "    through the fit method) if sample_weight is specified.\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "       class_weight == 'balanced'\n",
       "\n",
       "n_jobs : int, default=None\n",
       "    Number of CPU cores used during the cross-validation loop.\n",
       "    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
       "    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
       "    for more details.\n",
       "\n",
       "verbose : int, default=0\n",
       "    For the 'liblinear', 'sag' and 'lbfgs' solvers set verbose to any\n",
       "    positive number for verbosity.\n",
       "\n",
       "refit : bool, default=True\n",
       "    If set to True, the scores are averaged across all folds, and the\n",
       "    coefs and the C that corresponds to the best score is taken, and a\n",
       "    final refit is done using these parameters.\n",
       "    Otherwise the coefs, intercepts and C that correspond to the\n",
       "    best scores across folds are averaged.\n",
       "\n",
       "intercept_scaling : float, default=1\n",
       "    Useful only when the solver 'liblinear' is used\n",
       "    and self.fit_intercept is set to True. In this case, x becomes\n",
       "    [x, self.intercept_scaling],\n",
       "    i.e. a \"synthetic\" feature with constant value equal to\n",
       "    intercept_scaling is appended to the instance vector.\n",
       "    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
       "\n",
       "    Note! the synthetic feature weight is subject to l1/l2 regularization\n",
       "    as all other features.\n",
       "    To lessen the effect of regularization on synthetic feature weight\n",
       "    (and therefore on the intercept) intercept_scaling has to be increased.\n",
       "\n",
       "multi_class : {'auto, 'ovr', 'multinomial'}, default='auto'\n",
       "    If the option chosen is 'ovr', then a binary problem is fit for each\n",
       "    label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
       "    across the entire probability distribution, *even when the data is\n",
       "    binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
       "    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
       "    and otherwise selects 'multinomial'.\n",
       "\n",
       "    .. versionadded:: 0.18\n",
       "       Stochastic Average Gradient descent solver for 'multinomial' case.\n",
       "    .. versionchanged:: 0.22\n",
       "        Default changed from 'ovr' to 'auto' in 0.22.\n",
       "\n",
       "random_state : int, RandomState instance, default=None\n",
       "    Used when `solver='sag'`, 'saga' or 'liblinear' to shuffle the data.\n",
       "    Note that this only applies to the solver and not the cross-validation\n",
       "    generator. See :term:`Glossary <random_state>` for details.\n",
       "\n",
       "l1_ratios : list of float, default=None\n",
       "    The list of Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``.\n",
       "    Only used if ``penalty='elasticnet'``. A value of 0 is equivalent to\n",
       "    using ``penalty='l2'``, while 1 is equivalent to using\n",
       "    ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a combination\n",
       "    of L1 and L2.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "classes_ : ndarray of shape (n_classes, )\n",
       "    A list of class labels known to the classifier.\n",
       "\n",
       "coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
       "    Coefficient of the features in the decision function.\n",
       "\n",
       "    `coef_` is of shape (1, n_features) when the given problem\n",
       "    is binary.\n",
       "\n",
       "intercept_ : ndarray of shape (1,) or (n_classes,)\n",
       "    Intercept (a.k.a. bias) added to the decision function.\n",
       "\n",
       "    If `fit_intercept` is set to False, the intercept is set to zero.\n",
       "    `intercept_` is of shape(1,) when the problem is binary.\n",
       "\n",
       "Cs_ : ndarray of shape (n_cs)\n",
       "    Array of C i.e. inverse of regularization parameter values used\n",
       "    for cross-validation.\n",
       "\n",
       "l1_ratios_ : ndarray of shape (n_l1_ratios)\n",
       "    Array of l1_ratios used for cross-validation. If no l1_ratio is used\n",
       "    (i.e. penalty is not 'elasticnet'), this is set to ``[None]``\n",
       "\n",
       "coefs_paths_ : ndarray of shape (n_folds, n_cs, n_features) or                    (n_folds, n_cs, n_features + 1)\n",
       "    dict with classes as the keys, and the path of coefficients obtained\n",
       "    during cross-validating across each fold and then across each Cs\n",
       "    after doing an OvR for the corresponding class as values.\n",
       "    If the 'multi_class' option is set to 'multinomial', then\n",
       "    the coefs_paths are the coefficients corresponding to each class.\n",
       "    Each dict value has shape ``(n_folds, n_cs, n_features)`` or\n",
       "    ``(n_folds, n_cs, n_features + 1)`` depending on whether the\n",
       "    intercept is fit or not. If ``penalty='elasticnet'``, the shape is\n",
       "    ``(n_folds, n_cs, n_l1_ratios_, n_features)`` or\n",
       "    ``(n_folds, n_cs, n_l1_ratios_, n_features + 1)``.\n",
       "\n",
       "scores_ : dict\n",
       "    dict with classes as the keys, and the values as the\n",
       "    grid of scores obtained during cross-validating each fold, after doing\n",
       "    an OvR for the corresponding class. If the 'multi_class' option\n",
       "    given is 'multinomial' then the same scores are repeated across\n",
       "    all classes, since this is the multinomial class. Each dict value\n",
       "    has shape ``(n_folds, n_cs`` or ``(n_folds, n_cs, n_l1_ratios)`` if\n",
       "    ``penalty='elasticnet'``.\n",
       "\n",
       "C_ : ndarray of shape (n_classes,) or (n_classes - 1,)\n",
       "    Array of C that maps to the best scores across every class. If refit is\n",
       "    set to False, then for each class, the best C is the average of the\n",
       "    C's that correspond to the best scores for each fold.\n",
       "    `C_` is of shape(n_classes,) when the problem is binary.\n",
       "\n",
       "l1_ratio_ : ndarray of shape (n_classes,) or (n_classes - 1,)\n",
       "    Array of l1_ratio that maps to the best scores across every class. If\n",
       "    refit is set to False, then for each class, the best l1_ratio is the\n",
       "    average of the l1_ratio's that correspond to the best scores for each\n",
       "    fold.  `l1_ratio_` is of shape(n_classes,) when the problem is binary.\n",
       "\n",
       "n_iter_ : ndarray of shape (n_classes, n_folds, n_cs) or (1, n_folds, n_cs)\n",
       "    Actual number of iterations for all classes, folds and Cs.\n",
       "    In the binary or multinomial cases, the first dimension is equal to 1.\n",
       "    If ``penalty='elasticnet'``, the shape is ``(n_classes, n_folds,\n",
       "    n_cs, n_l1_ratios)`` or ``(1, n_folds, n_cs, n_l1_ratios)``.\n",
       "\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.datasets import load_iris\n",
       ">>> from sklearn.linear_model import LogisticRegressionCV\n",
       ">>> X, y = load_iris(return_X_y=True)\n",
       ">>> clf = LogisticRegressionCV(cv=5, random_state=0).fit(X, y)\n",
       ">>> clf.predict(X[:2, :])\n",
       "array([0, 0])\n",
       ">>> clf.predict_proba(X[:2, :]).shape\n",
       "(2, 3)\n",
       ">>> clf.score(X, y)\n",
       "0.98...\n",
       "\n",
       "See Also\n",
       "--------\n",
       "LogisticRegression\n",
       "\u001b[0;31mFile:\u001b[0m           /data_01/script/script_python/.venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LogisticRegressionCV?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "6cbea431-84f5-49e9-8ae0-b64510d7213c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data_01/script/script_python/.venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/data_01/script/script_python/.venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/data_01/script/script_python/.venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/data_01/script/script_python/.venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/data_01/script/script_python/.venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/data_01/script/script_python/.venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/data_01/script/script_python/.venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/data_01/script/script_python/.venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/data_01/script/script_python/.venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/data_01/script/script_python/.venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8080808080808081"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model5\n",
    "# randomforest\n",
    "lrcv01 = LogisticRegressionCV(random_state=0, max_iter=300, cv=3)\n",
    "\n",
    "lrcv01.fit(X_train1, y_train1)\n",
    "\n",
    "# 精度検証\n",
    "lrcv01.score(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b9d67fdf-7962-40ae-b3ae-e252b2a650b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data_01/script/script_python/.venv/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/data_01/script/script_python/.venv/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/data_01/script/script_python/.venv/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/data_01/script/script_python/.venv/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/data_01/script/script_python/.venv/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# 交差検証\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 分析結果比較\n",
    "scores_01 = cross_val_score(lr ,X_train1, y_train1)\n",
    "# print('Cross-Validation scores: {}'.format(scores_01))\n",
    "\n",
    "scores_02 = cross_val_score(xgb01 ,X_train1, y_train1)\n",
    "# print('Cross-Validation scores: {}'.format(scores_02))\n",
    "\n",
    "scores_03 = cross_val_score(rf02 ,X_train1, y_train1)\n",
    "# print('Cross-Validation scores: {}'.format(scores_02))\n",
    "\n",
    "scores_04 = cross_val_score(lrcv01 ,X_train1, y_train1)\n",
    "# print('Cross-Validation scores: {}'.format(scores_02))\n",
    "\n",
    "# スコアの平均値\n",
    "# import numpy as np\n",
    "# print('Average score: {}'.format(np.mean(scores_01)))\n",
    "# print('Average score: {}'.format(np.mean(scores_02)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "015ea6eb-4f73-4d75-a640-9a66c48f2b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "607243f8-fde1-43e7-8bef-14e9ec7a8488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data_01/script/script_python/.venv/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/data_01/script/script_python/.venv/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#cv_scores = cross_validate(gnb, iris.data, iris.target,\n",
    "#                            cv=kf, scoring=scoring)\n",
    "\n",
    "# 分析結果比較\n",
    "scores_criteria = ['balanced_accuracy', 'precision', 'recall', 'f1']  # 2値分類で有効そうな指標\n",
    "cv_scores_01 = cross_validate(lr ,X_train1, y_train1, cv=3, scoring=scores_criteria)\n",
    "\n",
    "cv_scores_02 = cross_validate(xgb01 ,X_train1, y_train1, cv=3, scoring=scores_criteria)\n",
    "\n",
    "cv_scores_03 = cross_validate(rf02 ,X_train1, y_train1, cv=3, scoring=scores_criteria)\n",
    "\n",
    "cv_scores_04 = cross_validate(lrcv01 ,X_train1, y_train1, cv=3, scoring=scores_criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a77a2719-01fd-443e-aab8-2f8dcbee10c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seriesにcross_val_scoreを格納\n",
    "ser_score_01 = pd.Series(scores_01)\n",
    "ser_score_02 = pd.Series(scores_02)\n",
    "ser_score_03 = pd.Series(scores_03)\n",
    "ser_score_04 = pd.Series(scores_04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ccb7331e-a4f4-4cda-a4a8-2a0c03890fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seriesにcross_validateを格納\n",
    "ser_cv_score_01 = pd.Series(cv_scores_01)\n",
    "ser_cv_score_02 = pd.Series(cv_scores_02)\n",
    "ser_cv_score_03 = pd.Series(cv_scores_03)\n",
    "ser_cv_score_04 = pd.Series(cv_scores_04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ca204408-df70-4f6a-be1d-eb63a3e86af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossValSore_01の平均値 0.8198262355789773 CrossValScore_02の平均値, 0.8265774106252671\n"
     ]
    }
   ],
   "source": [
    "# cross_val_scoreの平均値\n",
    "# print(\"CrossValSore_01の平均値\", ser_score_01.mean(), \"CrossValScore_02の平均値,\", ser_score_02.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "9c4ca6bf-e31d-4a98-b032-79a338d04c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time                  [0.044014692306518555, 0.022696971893310547]\n",
      "score_time                 [0.017159700393676758, 0.00801396369934082]\n",
      "test_balanced_accuracy        [0.7450854700854701, 0.5032051282051282]\n",
      "test_precision                [0.7717391304347826, 0.3956043956043956]\n",
      "test_recall                   [0.6068376068376068, 0.9230769230769231]\n",
      "test_f1                       [0.6794258373205742, 0.5538461538461539]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(ser_cv_score_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "2f2df910-fcd9-4aba-90ab-6c556ff2428b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time                   [0.04112958908081055, 0.043048858642578125]\n",
      "score_time                [0.011833429336547852, 0.009324073791503906]\n",
      "test_balanced_accuracy       [0.6858974358974359, 0.49786324786324787]\n",
      "test_precision               [0.8333333333333334, 0.39285714285714285]\n",
      "test_recall                  [0.42735042735042733, 0.9401709401709402]\n",
      "test_f1                       [0.5649717514124294, 0.5541561712846348]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# pd.DataFrame(cv_scores_02)\n",
    "print(ser_cv_score_02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "0b70e63f-7b94-4d63-8916-a96310947c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossValidate_01の平均値 [0.47737707 0.40107392] CrossValidate_02の平均値, [0.42741933 0.40623674]\n"
     ]
    }
   ],
   "source": [
    "# pd.DataFrame(cv_scores_03)\n",
    "print(ser_cv_score_03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa83d5b-4b82-4fde-8938-41fb744445b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(cv_scores_02)\n",
    "print(ser_cv_score_04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "cea2e6d2-a7c2-4db2-b66f-ac9b01d43f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測用ファイルの読み込み\n",
    "TEST_DATA = 'test.csv'\n",
    "predict_data = pd.read_csv(os.path.join(DATA_DIR, TEST_DATA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "7947210c-eb8a-4c5a-babb-fa121d91cd04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>3</td>\n",
       "      <td>Spector, Mr. Woolf</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A.5. 3236</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "      <td>Oliva y Ocana, Dona. Fermina</td>\n",
       "      <td>female</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17758</td>\n",
       "      <td>108.9000</td>\n",
       "      <td>C105</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>3</td>\n",
       "      <td>Saether, Mr. Simon Sivertsen</td>\n",
       "      <td>male</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SOTON/O.Q. 3101262</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>3</td>\n",
       "      <td>Ware, Mr. Frederick</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>359309</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>3</td>\n",
       "      <td>Peter, Master. Michael J</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2668</td>\n",
       "      <td>22.3583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Pclass                                          Name  \\\n",
       "0            892       3                              Kelly, Mr. James   \n",
       "1            893       3              Wilkes, Mrs. James (Ellen Needs)   \n",
       "2            894       2                     Myles, Mr. Thomas Francis   \n",
       "3            895       3                              Wirz, Mr. Albert   \n",
       "4            896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)   \n",
       "..           ...     ...                                           ...   \n",
       "413         1305       3                            Spector, Mr. Woolf   \n",
       "414         1306       1                  Oliva y Ocana, Dona. Fermina   \n",
       "415         1307       3                  Saether, Mr. Simon Sivertsen   \n",
       "416         1308       3                           Ware, Mr. Frederick   \n",
       "417         1309       3                      Peter, Master. Michael J   \n",
       "\n",
       "        Sex   Age  SibSp  Parch              Ticket      Fare Cabin Embarked  \n",
       "0      male  34.5      0      0              330911    7.8292   NaN        Q  \n",
       "1    female  47.0      1      0              363272    7.0000   NaN        S  \n",
       "2      male  62.0      0      0              240276    9.6875   NaN        Q  \n",
       "3      male  27.0      0      0              315154    8.6625   NaN        S  \n",
       "4    female  22.0      1      1             3101298   12.2875   NaN        S  \n",
       "..      ...   ...    ...    ...                 ...       ...   ...      ...  \n",
       "413    male   NaN      0      0           A.5. 3236    8.0500   NaN        S  \n",
       "414  female  39.0      0      0            PC 17758  108.9000  C105        C  \n",
       "415    male  38.5      0      0  SOTON/O.Q. 3101262    7.2500   NaN        S  \n",
       "416    male   NaN      0      0              359309    8.0500   NaN        S  \n",
       "417    male   NaN      1      1                2668   22.3583   NaN        C  \n",
       "\n",
       "[418 rows x 11 columns]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "e07db312-de7a-4c78-9450-dc724f0416c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測用ファイルをモデリングに使った学習用・テスト用ファイルに合わせ修正"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "024ef751-8fa3-43fd-87a1-518bf3f1e150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((418, 11), (594, 11), (594,), (297,))"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_data.shape, X_train1.shape, y_train1.shape, y_test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "e86c4680-da0a-4c55-8e63-f44d88c618e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測用ファイルを修正 \n",
    "# 性別をダミー変数化\n",
    "predict_data['Sex'] = predict_data['Sex'].replace(['male', 'female'], [0, 1])\n",
    "\n",
    "# カテゴリカルデータをダミー変数化\n",
    "embarked_le_pred = le.fit_transform(predict_data['Embarked'].values)\n",
    "\n",
    "name_le_pred = le.fit_transform(predict_data['Name'].values)\n",
    "\n",
    "ticket_le_pred = le.fit_transform(predict_data['Ticket'].values)\n",
    "\n",
    "cabin_le_pred = le.fit_transform(predict_data['Cabin'].values)\n",
    "\n",
    "# ダミー変数化したデータを元データに適用\n",
    "# data = DATA\n",
    "predict_data['Embarked'] = embarked_le_pred\n",
    "predict_data['Name'] = name_le_pred\n",
    "predict_data['Ticket'] = ticket_le_pred\n",
    "predict_data['Cabin'] = cabin_le_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "81845da7-73c6-484f-80ca-e17b521f56a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(418, 11)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "26c2a885-de46-48f2-a4b1-ffc6ae3857e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pred01 = xgb01.predict(predict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "437c622e-f084-4224-8fc6-3eef081e17a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提出用ファイル読み込み\n",
    "SUBMIT_DATA = 'gender_submission.csv'\n",
    "data_submit = pd.read_csv(os.path.join(DATA_DIR, SUBMIT_DATA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "33179951-31d1-4760-8f0d-8d52f54f9760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提出用ファイルに予測結果を連結\n",
    "data_submit['Survived'] = xgb_pred01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "5e001b70-41b9-47ca-9dba-76aa9f646eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write csv\n",
    "data_submit.to_csv(os.path.join(DATA_DIR, 'submission_xgb06.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092317d0-5e83-4528-ac08-5689e79f63b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
